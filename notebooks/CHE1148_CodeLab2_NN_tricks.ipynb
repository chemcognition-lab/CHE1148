{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Installs"
      ],
      "metadata": {
        "id": "TAmlj3zhsyMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install umap-learn\n",
        "!pip install git+https://github.com/bp-kelley/descriptastorus\n",
        "!pip install rdkit\n",
        "!pip install torchinfo\n",
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "D6kYOzgWG7HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Import libraries"
      ],
      "metadata": {
        "id": "ZMW5SA-qG9gG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5mkmn3cAjqa"
      },
      "outputs": [],
      "source": [
        "# Standard python libraries\n",
        "import collections\n",
        "import random\n",
        "import itertools\n",
        "import io\n",
        "\n",
        "# Scientific python\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "import scipy as sp\n",
        "import scipy.spatial.distance as sp_dist\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "\n",
        "# Chemoinformatics\n",
        "import rdkit\n",
        "import rdkit.Chem.Descriptors\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from rdkit.Chem import Draw\n",
        "from rdkit.Chem.Draw.MolDrawing import MolDrawing\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "from rdkit.Chem import Descriptors\n",
        "from rdkit.Chem import rdFingerprintGenerator\n",
        "import descriptastorus\n",
        "# Great progress bar\n",
        "import tqdm.auto as tqdm\n",
        "# ML: UMAP, PCA and clustering\n",
        "import umap\n",
        "import sklearn.decomposition\n",
        "import sklearn.cluster\n",
        "import sklearn.model_selection\n",
        "import sklearn.preprocessing\n",
        "\n",
        "# Our DL stack\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as torch_data\n",
        "import torchinfo\n",
        "import torchmetrics\n",
        "\n",
        "# Jupyter/IPython libraries\n",
        "import IPython.display as ipy_display\n",
        "from IPython.display import Image\n",
        "import base64\n",
        "import PIL\n",
        "\n",
        "for mod in [np, sp, rdkit, torch]:\n",
        "    print(f'{mod.__name__:20s}:{mod.__version__}')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"\\nUsing device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "3aGw6fHZs-lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample dataset, solubility (logP) ðŸ’§\n",
        "\n",
        "Datatset source: Delaney's solubility dataset from ESOL:â€‰ Estimating Aqueous Solubility Directly from Molecular Structure\n",
        "(https://pubs.acs.org/doi/10.1021/ci034243x)\n"
      ],
      "metadata": {
        "id": "_LMD0aUhpYOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://raw.githubusercontent.com/chemcognition-lab/CHE1148/refs/heads/main/data/delaney/delaney-solubility.csv"
      ],
      "metadata": {
        "id": "HW7h_HMLpb3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('delaney-solubility.csv')\n",
        "target_col = 'measured log solubility in mols per litre'\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "WRJXwOYDpfaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate our input features"
      ],
      "metadata": {
        "id": "i1P-3aujyaUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['mol'] = df['smiles'].apply(Chem.MolFromSmiles)\n",
        "generator = descriptastorus.descriptors.rdNormalizedDescriptors.RDKit2DNormalized()\n",
        "x_all = np.stack([generator.calculateMol(m, None) for m in tqdm.tqdm(df['mol'].tolist())])\n",
        "y_all = df[target_col].values.reshape(-1, 1)\n",
        "print(f\"Features: {x_all.shape}, Targets: {y_all.shape}\")"
      ],
      "metadata": {
        "id": "O4lRgsBGyWuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create splits\n",
        "\n",
        "We generate three columns in the DataFrame indicating 'train' (80%) or 'test' (20%).\n",
        "\n",
        "Later, the 'train' portion will be further split into Train (65%) and Val (15%).\n",
        "\n",
        "\n",
        "### Random first"
      ],
      "metadata": {
        "id": "oRNxikfYyx9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = len(df)\n",
        "indices = list(range(n))\n",
        "test_size = 0.20\n",
        "df['split_random_new'] = 'train'\n",
        "test_idxs = np.random.choice(n, int(n * test_size), replace=False)\n",
        "df.iloc[test_idxs, df.columns.get_loc('split_random_new')] = 'test'\n",
        "df['split_random_new'] = 'train'\n",
        "test_idxs = np.random.choice(n, int(n * test_size), replace=False)\n",
        "df.iloc[test_idxs, df.columns.get_loc('split_random_new')] = 'test'"
      ],
      "metadata": {
        "id": "JQ-QbK-oyyHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Stratified Split (High LogP Threshold)\n",
        "\n",
        "Binarize based on top 20% solubility (High vs Low/Medium)"
      ],
      "metadata": {
        "id": "TQQB8k1o0Eop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = df[target_col].quantile(0.8)\n",
        "df['strat_bin_new'] = (df[target_col] > threshold).astype(int)\n",
        "splitter = sklearn.model_selection.StratifiedShuffleSplit(n_splits=1,\n",
        "                                                          test_size=test_size,\n",
        "                                                          random_state=42)\n",
        "train_idx, test_idx = next(splitter.split(indices, df['strat_bin']))\n",
        "df['split_stratified_new'] = 'train'\n",
        "df.iloc[test_idx, df.columns.get_loc('split_stratified_new')] = 'test'"
      ],
      "metadata": {
        "id": "Mhkz9fX00Evy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Adversarial Split (Distance/Cluster based)\n",
        "\n",
        "We cluster the data. We explicitly assign specific clusters to 'Test'\n",
        "to force the model to predict on chemically distinct groups (OOD)."
      ],
      "metadata": {
        "id": "g4_5G8Mc0VtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = sklearn.cluster.KMeans(n_clusters=10, random_state=42, n_init=10)\n",
        "clusters = kmeans.fit_predict(x_all)\n",
        "# Pick clusters to define the test set until we reach ~20%\n",
        "cluster_counts = pd.Series(clusters).value_counts()\n",
        "test_clusters = []\n",
        "current_count = 0\n",
        "target_count = int(n * test_size)\n",
        "\n",
        "for clus_id, count in cluster_counts.items():\n",
        "    if current_count + count <= target_count + 50: # Allow small buffer\n",
        "        test_clusters.append(clus_id)\n",
        "        current_count += count\n",
        "\n",
        "df['split_adversarial_new'] = 'train'\n",
        "test_mask = np.isin(clusters, test_clusters)\n",
        "df.loc[test_mask, 'split_adversarial_new'] = 'test'"
      ],
      "metadata": {
        "id": "fUU82by00WEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization of the Adversarial Split\n"
      ],
      "metadata": {
        "id": "ftbwUfEC05u3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = sklearn.decomposition.PCA(n_components=2)\n",
        "x_pca = pca.fit_transform(x_all)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(x=x_pca[:,0], y=x_pca[:,1], hue=df['split_adversarial'], style=df['split_adversarial'])\n",
        "plt.title(\"Adversarial Split (PCA View)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QLSmBvDp04zT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loaders Setup"
      ],
      "metadata": {
        "id": "HbDAybyC1JgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataloaders(split_name, x_data, y_data, df, batch_size=64):\n",
        "    \"\"\"\n",
        "    1. Selects 80% Train / 20% Test based on `split_name` column.\n",
        "    2. Splits the 80% Train into -> 65% Train / 15% Val.\n",
        "    \"\"\"\n",
        "\n",
        "    # Indices for the outer split\n",
        "    train_val_indices = df[df[split_name] == 'train'].index.to_numpy()\n",
        "    test_indices = df[df[split_name] == 'test'].index.to_numpy()\n",
        "\n",
        "    # Create the internal split (Train vs Val)\n",
        "    # We want Val to be 15% of TOTAL, and Train to be 65% of TOTAL.\n",
        "    # Total Train+Val is 80%.\n",
        "    # Fraction for val = 15 / 80 = 0.1875\n",
        "    val_rel_size = 15 / 80\n",
        "\n",
        "    train_idx, val_idx = sklearn.model_selection.train_test_split(\n",
        "        train_val_indices, test_size=val_rel_size, random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"Split: {split_name}\")\n",
        "    print(f\"Train: {len(train_idx)} (65%) | Val: {len(val_idx)} (15%) | Test: {len(test_indices)} (20%)\")\n",
        "\n",
        "    # Convert to Tensors\n",
        "    x_tensor = torch.tensor(x_data, dtype=torch.float32)\n",
        "    y_tensor = torch.tensor(y_data, dtype=torch.float32)\n",
        "\n",
        "    train_ds = torch_data.TensorDataset(x_tensor[train_idx], y_tensor[train_idx])\n",
        "    val_ds = torch_data.TensorDataset(x_tensor[val_idx], y_tensor[val_idx])\n",
        "    test_ds = torch_data.TensorDataset(x_tensor[test_indices], y_tensor[test_indices])\n",
        "\n",
        "    return (\n",
        "        torch_data.DataLoader(train_ds, batch_size=batch_size, shuffle=True),\n",
        "        torch_data.DataLoader(val_ds, batch_size=batch_size, shuffle=False),\n",
        "        torch_data.DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "    )"
      ],
      "metadata": {
        "id": "3p37q2ut1Jm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SPLIT = 'split_random'\n",
        "train_loader, val_loader, test_loader = get_dataloaders(SPLIT, x_all, y_all, df)"
      ],
      "metadata": {
        "id": "jigV-KsW1V67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up a model class"
      ],
      "metadata": {
        "id": "CUWD9sRzCeNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_dim=1, dropout=0.0):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        # Input\n",
        "        self.layers.append(nn.Linear(input_size, hidden_size))\n",
        "        self.layers.append(nn.ReLU())\n",
        "        self.layers.append(nn.LayerNorm(hidden_size))\n",
        "\n",
        "        # Hidden\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
        "            self.layers.append(nn.ReLU())\n",
        "            self.layers.append(nn.Dropout(p=dropout))\n",
        "            self.layers.append(nn.LayerNorm(hidden_size))\n",
        "\n",
        "        # Output\n",
        "        self.predict = nn.Linear(hidden_size, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return self.predict(x)"
      ],
      "metadata": {
        "id": "4WWAewMeA-6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create the model based on some hyper-parameters"
      ],
      "metadata": {
        "id": "-jJ-dukjS588"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = x_all.shape[1]\n",
        "model = MLP(input_dim, hidden_size=128, num_layers=3)\n",
        "model = model.to(device)\n",
        "torchinfo.summary(model, input_size=(64, input_dim))"
      ],
      "metadata": {
        "id": "ilLKDEHIBDTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimization & Metrics\n"
      ],
      "metadata": {
        "id": "pJvRtcoV10Q9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, verbose=False):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss:\n",
        "            self.counter += 1\n",
        "            if self.verbose and self.counter % 5 == 0:\n",
        "                print(f\"EarlyStopping counter: {self.counter}/{self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n"
      ],
      "metadata": {
        "id": "cai-DPCw10YA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our metrics"
      ],
      "metadata": {
        "id": "rhqoWMgq2GXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "metrics_template = torchmetrics.MetricCollection({\n",
        "    'r2': torchmetrics.R2Score(),\n",
        "    'mae': torchmetrics.MeanAbsoluteError(),\n",
        "    'spear': torchmetrics.SpearmanCorrCoef(),\n",
        "})\n",
        "\n",
        "train_metrics = metrics_template.clone().to(device)\n",
        "val_metrics = metrics_template.clone().to(device)\n",
        "test_metrics = metrics_template.clone().to(device)"
      ],
      "metadata": {
        "id": "3svNrTdk2GqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.001\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "early_stopping = EarlyStopping(patience=15, verbose=True)\n",
        "clip_value = 1.0"
      ],
      "metadata": {
        "id": "ARD49k7rBD6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation & Training loop"
      ],
      "metadata": {
        "id": "Q8Qpixef2NWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader, metrics_obj):\n",
        "    model.eval()\n",
        "    metrics_obj.reset()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            preds = model(x)\n",
        "            loss = loss_fn(preds, y)\n",
        "            total_loss += loss.item()\n",
        "            metrics_obj.update(preds, y)\n",
        "\n",
        "    results = metrics_obj.compute()\n",
        "    results = {k: v.item() for k,v in results.items()}\n",
        "    results['loss'] = total_loss / len(loader)\n",
        "    return results"
      ],
      "metadata": {
        "id": "6CQvPxJd2QSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "history = []\n",
        "\n",
        "pbar = tqdm.tqdm(range(1, epochs+1))\n",
        "\n",
        "for epoch in pbar:\n",
        "    # --- TRAIN ---\n",
        "    model.train()\n",
        "    train_metrics.reset()\n",
        "    train_loss_acc = 0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(x)\n",
        "        loss = loss_fn(preds, y)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss_acc += loss.item()\n",
        "        train_metrics.update(preds, y)\n",
        "\n",
        "    # --- EVALUATE ---\n",
        "    train_res = train_metrics.compute()\n",
        "    train_res = {k: v.item() for k,v in train_res.items()}\n",
        "    train_res['loss'] = train_loss_acc / len(train_loader)\n",
        "\n",
        "    val_res = evaluate(model, val_loader, val_metrics)\n",
        "\n",
        "    # Log\n",
        "    cur_lr = optimizer.param_groups[0]['lr']\n",
        "    history.append({'epoch': epoch, 'split': 'train', 'lr': cur_lr} | train_res)\n",
        "    history.append({'epoch': epoch, 'split': 'val', 'lr': cur_lr} | val_res)\n",
        "\n",
        "    # Scheduler & Early Stopping\n",
        "    scheduler.step(val_res['loss'])\n",
        "    early_stopping(val_res['loss'])\n",
        "\n",
        "    pbar.set_postfix({'t_loss': f\"{train_res['loss']:.2f}\",\n",
        "                      'v_loss': f\"{val_res['loss']:.2f}\",\n",
        "                      'v_r2': f\"{val_res['r2']:.2f}\"})\n",
        "\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break\n",
        "\n",
        "# DataFrame History\n",
        "df_hist = pd.DataFrame(history)\n",
        "df_hist"
      ],
      "metadata": {
        "id": "3sWfufiQBJGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training dinamics"
      ],
      "metadata": {
        "id": "6Bep_WFm3faV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_to_plot = {\n",
        "    'loss': 'MSE Loss',\n",
        "    'r2': 'R2 Score',\n",
        "    'spear': 'Spearman'\n",
        "}\n",
        "\n",
        "for y_col, title in metrics_to_plot.items():\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.lineplot(data=df_hist, x='epoch', y=y_col, hue='split')\n",
        "    plt.title(title)\n",
        "\n",
        "    # Specific styling for R2\n",
        "    if y_col == 'r2':\n",
        "        plt.ylim(0, 1)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "xKpfDFbS3fiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "e5y0MM614KCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_res = evaluate(model, test_loader, test_metrics)\n",
        "print(\"\\nFinal Test Set Results:\")\n",
        "for k, v in test_res.items():\n",
        "    print(f\"{k:>10}: {v:.4f}\")\n",
        "\n",
        "# Compare distributions\n",
        "preds = []\n",
        "actuals = []\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        preds.extend(model(x.to(device)).cpu().numpy().flatten())\n",
        "        actuals.extend(y.numpy().flatten())\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(actuals, preds, alpha=0.5)\n",
        "plt.plot([min(actuals), max(actuals)], [min(actuals), max(actuals)], 'r--')\n",
        "plt.xlabel(\"Actual LogP\")\n",
        "plt.ylabel(\"Predicted LogP\")\n",
        "plt.title(f\"Test Set Parity Plot ({SPLIT})\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NqEpdwg_4KHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exhaustive Evaluation (Benchmarking)"
      ],
      "metadata": {
        "id": "y8Qn2dWt6K-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the splits to benchmark\n",
        "target_splits = ['split_random', 'split_stratified', 'split_adversarial']\n",
        "benchmark_results = []\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"STARTING BENCHMARK LOOP\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "for split_name in target_splits:\n",
        "    print(f\"\\n>>> Training on split: {split_name}\")\n",
        "\n",
        "    # 1. Get Loaders for this specific split\n",
        "    t_loader, v_loader, te_loader = get_dataloaders(split_name, x_all, y_all, df)\n",
        "\n",
        "    # 2. Re-initialize Model (Fresh Weights)\n",
        "    model_bench = MLP(input_dim, hidden_size=128, num_layers=3, dropout=0.2).to(device)\n",
        "\n",
        "    # 3. Re-initialize Optimization tools\n",
        "    optimizer_bench = torch.optim.AdamW(model_bench.parameters(), lr=0.001, weight_decay=1e-2)\n",
        "    scheduler_bench = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_bench, mode='min', factor=0.5, patience=5)\n",
        "    early_stopping_bench = EarlyStopping(patience=15, verbose=False)\n",
        "\n",
        "    # Metrics (reset for this run)\n",
        "    bench_train_metrics = metrics_template.clone().to(device)\n",
        "    bench_val_metrics = metrics_template.clone().to(device)\n",
        "    bench_test_metrics = metrics_template.clone().to(device)\n",
        "\n",
        "    # 4. Training Loop\n",
        "    # We use a silent loop (no progress bar) to keep output clean,\n",
        "    # or a simple tqdm if desired.\n",
        "    for epoch in tqdm.tqdm(range(100), desc=f\"Ep {split_name}\", leave=False):\n",
        "        model_bench.train()\n",
        "        bench_train_metrics.reset()\n",
        "\n",
        "        for x, y in t_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer_bench.zero_grad()\n",
        "            preds = model_bench(x)\n",
        "            loss = loss_fn(preds, y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model_bench.parameters(), 1.0)\n",
        "            optimizer_bench.step()\n",
        "\n",
        "        # Validation\n",
        "        val_res_bench = evaluate(model_bench, v_loader, bench_val_metrics)\n",
        "\n",
        "        # Scheduler & ES\n",
        "        scheduler_bench.step(val_res_bench['loss'])\n",
        "        early_stopping_bench(val_res_bench['loss'])\n",
        "\n",
        "        if early_stopping_bench.early_stop:\n",
        "            break\n",
        "\n",
        "    # 5. Final Evaluation on Test Set\n",
        "    test_res_bench = evaluate(model_bench, te_loader, bench_test_metrics)\n",
        "\n",
        "    # Store results\n",
        "    result_entry = {'Split Type': split_name}\n",
        "    result_entry.update(test_res_bench)\n",
        "    benchmark_results.append(result_entry)\n",
        "\n",
        "    print(f\"Finished {split_name}. Test R2: {test_res_bench['r2']:.4f}\")\n",
        "\n",
        "# --- Generate Table ---\n",
        "df_benchmark = pd.DataFrame(benchmark_results)\n",
        "df_benchmark"
      ],
      "metadata": {
        "id": "ycKgitbL6Gu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reorder columns for readability\n",
        "cols = ['Split Type', 'loss', 'mae', 'r2', 'spear']\n",
        "df_benchmark = df_benchmark[cols]\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"FINAL BENCHMARK RESULTS\")\n",
        "print(\"=\"*40)\n",
        "ipy_display.display(df_benchmark)\n",
        "\n",
        "# Optional: Visualize the comparison\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(data=df_benchmark, x='Split Type', y='r2', hue='Split Type', palette='Set2')\n",
        "plt.title(\"R2 Score across Data Splits\")\n",
        "plt.ylim(0, 1)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YIUv8fYH6Prr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}